{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPl6ZbbZR17UblcRVXuGSL8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sofiasilingardi16/Misogyny-Repeated-and-Reposted/blob/main/Coding_SUB_RQ_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Opie4gUQniq"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install transformers torch torchvision pandas pillow tqdm\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import clip\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoImageProcessor, SiglipForImageClassification\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your Drive if you haven't already\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define your paths\n",
        "TSV_PATH = \"/content/drive/MyDrive/MA THESIS/CODE FOR THESIS/test.tsv\"\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/MA THESIS/test_images\"\n",
        "\n",
        "# Load your data\n",
        "df = pd.read_csv(TSV_PATH, sep=\"\\t\", on_bad_lines='skip')\n",
        "df = df[df[\"label\"] == 1].reset_index(drop=True)\n",
        "\n",
        "# Create full image paths\n",
        "df[\"image_path_full\"] = df[\"file_name\"].apply(lambda f: os.path.join(IMAGE_DIR, f))"
      ],
      "metadata": {
        "id": "D8rlbOuwa-Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CLIP\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Load SIGLIP\n",
        "siglip_model_name = \"prithivMLmods/siglip2-x256-explicit-content\"\n",
        "siglip_model = SiglipForImageClassification.from_pretrained(siglip_model_name).to(device)\n",
        "siglip_processor = AutoImageProcessor.from_pretrained(siglip_model_name, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "LskB5CjpbRxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_clip_sim(text, img_path):\n",
        "    try:\n",
        "        image = clip_preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "        tokens = clip.tokenize([text]).to(device)\n",
        "        with torch.no_grad():\n",
        "            img_vec = clip_model.encode_image(image).float()\n",
        "            txt_vec = clip_model.encode_text(tokens).float()\n",
        "            img_vec /= img_vec.norm(dim=-1, keepdim=True)\n",
        "            txt_vec /= txt_vec.norm(dim=-1, keepdim=True)\n",
        "        return (img_vec @ txt_vec.T).item()\n",
        "    except Exception as e:\n",
        "        print(f\"[CLIP ERROR] {img_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def compute_explicitness(img_path):\n",
        "    try:\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        inputs = siglip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = siglip_model(**inputs).logits\n",
        "            probs = torch.nn.functional.softmax(logits, dim=1).squeeze().cpu().tolist()\n",
        "        return probs[3] + probs[4]  # Pornography + Enticing or Sensual\n",
        "    except Exception as e:\n",
        "        print(f\"[SIGLIP ERROR] {img_path}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "DRrnVzXlbYe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "\n",
        "df[\"clip_sim\"] = df.progress_apply(\n",
        "    lambda row: compute_clip_sim(row[\"text\"], row[\"image_path_full\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "df[\"explicitness_score\"] = df.progress_apply(\n",
        "    lambda row: compute_explicitness(row[\"image_path_full\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Drop rows where processing failed\n",
        "df_clean = df.dropna(subset=[\"clip_sim\", \"explicitness_score\"]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "XpA63J8QbduC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = df_clean[\"clip_sim\"].quantile(0.75)\n",
        "\n",
        "df_clean[\"alignment\"] = df_clean[\"clip_sim\"].apply(\n",
        "    lambda x: \"amplify\" if x >= threshold else \"obscure\"\n",
        ")"
      ],
      "metadata": {
        "id": "bZKUjchqbhX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.histplot(df_clean[\"clip_sim\"], bins=30, kde=True)\n",
        "plt.title(\"CLIP Similarity Distribution\")\n",
        "plt.xlabel(\"clip_sim\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IRWQyk79c-V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(data=df_clean, x=\"alignment\", y=\"explicitness_score\")\n",
        "plt.title(\"Explicitness Score by Alignment Category\")\n",
        "plt.xlabel(\"Alignment\")\n",
        "plt.ylabel(\"Explicitness Score (SIGLIP)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JDX7FXA_iach"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_clean[[\"clip_sim\", \"explicitness_score\"]].corr())"
      ],
      "metadata": {
        "id": "SZ3jNhJmidB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = pd.concat([\n",
        "    df_clean[df_clean[\"alignment\"] == \"amplify\"].nlargest(2, \"clip_sim\"),\n",
        "    df_clean[df_clean[\"alignment\"] == \"obscure\"].nsmallest(2, \"clip_sim\")\n",
        "])\n",
        "\n",
        "examples[[\"file_name\", \"text\", \"clip_sim\", \"explicitness_score\", \"alignment\"]].to_csv(\"manual_examples.csv\", index=False)"
      ],
      "metadata": {
        "id": "I_Uq4O_yifVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(examples.head())"
      ],
      "metadata": {
        "id": "u4Fu23ukjQ4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.to_csv(\"subrq3_clip_results.csv\", index=False)"
      ],
      "metadata": {
        "id": "Y8jHk7gnj5lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your path to Drive\n",
        "drive_path = \"/content/drive/MyDrive/MA THESIS/subrq3_clip_results.csv\"\n",
        "\n",
        "# Save it there\n",
        "df_clean.to_csv(drive_path, index=False)"
      ],
      "metadata": {
        "id": "h6q_DIQEkHWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(df_clean[\"clip_sim\"], bins=30, kde=True)\n",
        "plt.title(\"Distribution of CLIP Similarity\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hn4wcCJqlFtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(data=df_clean, x=\"alignment\", y=\"explicitness_score\")\n",
        "plt.title(\"Explicitness by Alignment\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L7yYOMxilH4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.groupby(\"alignment\")[\"explicitness_score\"].describe()"
      ],
      "metadata": {
        "id": "0wtFUAwVkRwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean[[\"clip_sim\", \"explicitness_score\"]].corr()"
      ],
      "metadata": {
        "id": "PLezhc1lkaTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/MA THESIS/manual_examples.csv\"\n",
        "\n",
        "examples[[\"file_name\", \"text\", \"clip_sim\", \"explicitness_score\", \"alignment\"]].to_csv(save_path, index=False)"
      ],
      "metadata": {
        "id": "y7Il6VtXrbrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: how to print manual_examples.csv\n",
        "\n",
        "import pandas as pd\n",
        "print(pd.read_csv(\"manual_examples.csv\"))"
      ],
      "metadata": {
        "id": "wzb6WuR3kq1l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}